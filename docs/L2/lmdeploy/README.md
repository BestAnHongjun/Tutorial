# LMDeploy量化部署进阶实践

![](./figures/topic.jpg)

## 1.大模型缓存推理技术

### 1.1 LLM前向推理过程回顾

首先回顾一下LLM的前向推理过程。为了突出本章重点内容，省略了一些关于位置编码、Normalize算子的计算细节。

对于现有输入token序列 $T_{1:n}=[t_1, t_2, ..., t_n]$ ，首先会经过嵌入层(Embedding Layer)得到嵌入向量，这同时也是LLM第1层block的输入：

$$X_{1:n}^{(1)}=\text{EMB}(T_{1:n})=[x_1^{(1)}, x_2^{(1)}, ..., x_n^{(1)}]$$

该嵌入向量会依次经过LLM若干层block。对于模型第 $l$ 层，输入嵌入向量 $X_{1:n}^{(l)}=[x_1^{(l)}, x_2^{(l)}, ..., x_n^{(l)}]$ ，会先经过三个线性变换矩阵  $W_q^{(l)}$ 、 $W_k^{(l)}$ 、 $W_v^{(l)}$ ，得到 $Q_{1:n}^{(l)}$ 、 $K_{1:n}^{(l)}$ 、 $V_{1:n}^{(l)}$ ，如下：

$$Q_{1:n}^{(l)}=W_q^{(l)} X_{1:n}^{(l)}=[q_1^{(l)}, q_2^{(l)}, ..., q_n^{(l)}]$$

$$K_{1:n}^{(l)}=W_k^{(l)} X_{1:n}^{(l)}=[k_1^{(l)}, k_2^{(l)}, ..., k_n^{(l)}]$$

$$V_{1:n}^{(l)}=W_v^{(l)} X_{1:n}^{(l)}=[v_1^{(l)}, v_2^{(l)}, ..., v_n^{(l)}]$$

经注意力汇聚过程，得到输出 $H_{1:n}^{(l)}$ ：

$$H_{1:n}^{(l)}=\frac{\text{softmax}(Q_{1:n}^{(l)\top}\cdot K_{1:n}^{(l)})}{\sqrt{d_k}}V_{1:n}^{(l)}=[h_1^{(l)}, h_2^{(l)}, ..., h_n^{(l)}]$$

经过前馈层，得到本层输出 $Y_{1:n}^l$ ，计算细节略：

$$Y_{1:n}^{(l)}=\text{FFN}(H_{1:n}^{(l)})=[y_1^{(l)}, y_2^{(l)}, ..., y_n^{(l)}]$$

$Y_{1:n}^{(l)}$ 会作为下一层block的输入 $X_{1:n}^{(l+1)}$ ，即：

$$X_{1:n}^{(l+1)} = Y_{1:n}^{(l)}$$

对于最后一层（第 $L$ 层）的输出，最后一个向量 $y_n^{(L)}$ 将会经过一个线性层，转换为一个词表长度vocab_size大小的概率向量。随后经过一定的采样策略，以贪心采样策略为例(每次都选概率最大)，生成新的token。

$$t_{n+1} = \text{argmax}(\text{Linear}(y_n^{(L)}))$$

### 1.2 KV Cache技术

新的 $t_{n+1}$ 会和 $T_{1:n}$ 组合，作为新一轮的输入，然后计算每一层新的 $Q_{1:n+1}^{(l)}$ 、 $K_{1:n+1}^{(l)}$ 、 $V_{1:n+1}^{(l)}$ ：

$$Q_{1:n+1}^{(l)}=W_q^{(l)} X_{1:n+1}^{(l)}=[q_1^{(l)}, q_2^{(l)}, ..., q_{n}^{(l)}, q_{n+1}^{(l)}]$$

$$K_{1:n+1}^{(l)}=W_k^{(l)} X_{1:n+1}^{(l)}=[k_1^{(l)}, k_2^{(l)}, ..., k_{n}^{(l)}, k_{n+1}^{(l)}]$$

$$V_{1:n+1}^{(l)}=W_v^{(l)} X_{1:n+1}^{(l)}=[v_1^{(l)}, v_2^{(l)}, ..., v_{n}^{(l)}, v_{n+1}^{(l)}]$$

我们发现，其中 $[q_1^{(l)}, q_2^{(l)}, ..., q_{n}^{(l)}]$ 、 $[k_1^{(l)}, k_2^{(l)}, ..., k_{n}^{(l)}]$ 和 $[v_1^{(l)}, v_2^{(l)}, ..., v_{n}^{(l)}]$ 在上一轮已经计算过一遍了，这里出现了**计算冗余**。为了避免冗余的计算，我们不妨把上一轮计算过的 $Q_{1:n}^{(l)}$ 、 $K_{1:n}^{(l)}$ 、 $V_{1:n}^{(l)}$ “**缓存**”起来，本轮计算变为如下形式：

$$Q_{1:n+1}^{(l)}=W_q^{(l)} X_{1:n+1}^{(l)}=[Q_{1:n}^{(l)}, W_q^{(l)}x_{n+1}^{(l)}]=[Q_{1:n}^{(l)}, q_{n+1}^{(l)}]$$

$$K_{1:n+1}^{(l)}=W_k^{(l)} X_{1:n+1}^{(l)}=[K_{1:n}^{(l)}, W_k^{(l)}x_{n+1}^{(l)}]=[K_{1:n}^{(l)}, k_{n+1}^{(l)}]$$

$$V_{1:n+1}^{(l)}=W_v^{(l)} X_{1:n+1}^{(l)}=[V_{1:n}^{(l)}, W_v^{(l)}x_{n+1}^{(l)}]=[V_{1:n}^{(l)}, v_{n+1}^{(l)}]$$

这样，我们就得到了“**QKV Cache**”缓存方法。

再来观察采样过程。对于第n+1轮迭代，虽然模型输出了 $[y_1^{(L)}, y_2^{(L)}, ..., y_{n+1}^{(L)}]$ ，但其实只有 $y_{n+1}^{(L)}$ 是我们需要的结果。往前推，其实我们在注意力汇聚时，只需要得到 $h_{n+1}^{(l)}$ 。

$$h_{n+1}^{(l)}=\frac{\text{softmax}(q_{n+1}^{(l)\top} \cdot K_{1:n+1}^{(l)})}{\sqrt{d_k}}V_{1:n+1}^{(l)}$$

也就是说，我们其实只需要 $q_{n+1}^{(l)}$ 、 $K_{1:n+1}^{(l)}$ 、 $V_{1:n+1}^{(l)}$ 。对 $Q_{1:n}^{(l)}$ 的缓存也是不必要的。

因此，在LLM推理过程中，我们通常会对 $K_{1:n}^{(l)}$ 、 $V_{1:n}^{(l)}$ 进行缓存，称为**KV Cache**技术，通过避免重复计算来提升模型推理速度。当然，这通常会带来额外的内存开销。


## 2.大模型量化技术

### 2.1 通用量化原理

### 2.2 AWQ量化技术

### 2.3 SmoothQuant量化技术

## 3.大模型外推技术

### 3.1 位置编码外推

### 3.2 注意力缩放外推

## 4.Function Calling功能

## 5.实验课

请参考[experiment.md](./experiments.md)。
